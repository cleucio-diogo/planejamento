{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cleucio-diogo/planejamento/blob/main/analise_bases_parciais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzSTLU5qRuFh",
        "outputId": "c7ea5ded-cd6e-408e-ce58-49b8c52a52cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Importe a biblioteca tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp  # Importe o módulo subplots\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "raiz = '/content/drive'\n",
        "drive.mount(raiz)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### BLOCO DE FUNÇÕES AUXILIARES ####################\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/csv_files/\"\n",
        "input_path = file_path+'input/'\n",
        "output_path = file_path+'output/'\n",
        "\n",
        "import hashlib\n",
        "\n",
        "def gerar_identificador_unico(texto, comprimento=6):\n",
        "    \"\"\"\n",
        "    Gera um identificador único de até 'comprimento' caracteres a partir de uma string.\n",
        "\n",
        "    Parâmetros:\n",
        "    texto (str): A string de entrada para gerar o identificador.\n",
        "    comprimento (int): O número de caracteres desejados para o identificador (padrão é 6).\n",
        "\n",
        "    Retorna:\n",
        "    str: Um identificador único com o número de caracteres especificado.\n",
        "    \"\"\"\n",
        "    contador = 0\n",
        "    while True:\n",
        "        # Gerar um hash SHA-256 do texto concatenado com o contador\n",
        "        hash_object = hashlib.sha256((texto + str(contador)).encode())\n",
        "        # Converter o hash para hexadecimal\n",
        "        hash_hex = hash_object.hexdigest()\n",
        "        # Retornar os primeiros 'comprimento' caracteres do hash\n",
        "        identificador = hash_hex[:comprimento]\n",
        "\n",
        "        # Retornar o identificador\n",
        "        return identificador\n",
        "\n",
        "        contador += 1  # Incrementar o contador para a próxima tentativa\n",
        "\n",
        "def export_to_csv(dataframe, file_path,file_name=None, index=False, sep=';', encoding='utf-8'):\n",
        "    \"\"\"\n",
        "    Exporta um DataFrame para um arquivo CSV.\n",
        "\n",
        "    Parameters:\n",
        "    - dataframe: DataFrame a ser exportado.\n",
        "    - file_path: Caminho do arquivo onde o CSV será salvo. Se não fornecido, usa o nome do DataFrame.\n",
        "    - index: Se True, inclui o índice do DataFrame no CSV.\n",
        "    - sep: Separador a ser usado no CSV.\n",
        "    - encoding: Codificação a ser usada no CSV.\n",
        "    \"\"\"\n",
        "    if file_name is None:\n",
        "        # Usar o nome do DataFrame como o nome do arquivo\n",
        "        dataframe_name = [name for name, var in globals().items() if var is dataframe][0]\n",
        "        file_name = f\"{dataframe_name}.csv\"\n",
        "\n",
        "        # Criar o diretório se ele não existir\n",
        "        os.makedirs(file_path, exist_ok=True)\n",
        "\n",
        "        # Montar o caminho completo do arquivo\n",
        "        full_path = os.path.join(file_path, file_name)\n",
        "    else:\n",
        "        full_path = file_path + file_name\n",
        "    dataframe.to_csv(full_path, index=index, sep=sep, encoding=encoding)\n",
        "\n",
        "def limpar_colunas(df, colunas):\n",
        "  for col in colunas:\n",
        "    #() => permite quebras de linhas quando você quiser, sem afetar o código. Isso é ótimo para inserir comentários em pedaços de código que deveriam ficar em uma única linha\n",
        "    df[col] = (\n",
        "      df[col].apply(str).str.lower() #padronizando e transformando tudo em minusculas;\n",
        "      .str.replace('\\n','') #removendo quebras de linhas\n",
        "      .str.replace('r$','') #removendo r$\n",
        "      .str.replace('.', '', regex=False) #removendo separadores de milhar\n",
        "      .str.replace(',', '.', regex=False) # trocando o separador de dezena\n",
        "      .str.replace('nan','')\n",
        "      .str.strip() #removendo espaços em branco\n",
        "    )\n",
        "\n",
        "def to_numeric(df, colunas):\n",
        "  for col in colunas:\n",
        "    df[col] = (\n",
        "        pd.to_numeric(df[col], errors='coerce').astype(float)\n",
        "    ).fillna(0)\n",
        "\n",
        "def criar_faixas_valor(df, colunas, metodo='sturges', implace = True):\n",
        "  \"\"\"\n",
        "  Cria faixas de valor para colunas específicas em um DataFrame.\n",
        "\n",
        "  Parâmetros:\n",
        "  df (pd.DataFrame): DataFrame contendo os dados.\n",
        "  colunas (list): Lista de nomes de colunas para criar as faixas de valor.\n",
        "  metodo (str): Método para determinar o número de classes ('sturges' ou 'sqrt').\n",
        "\n",
        "  Retorna:\n",
        "  implace= True -> pd.DataFrame: dataframe original com as novas colunas de faixas de valor.\n",
        "  implace= False -> pd.DataFrame: inclui no dataframe original as novas colunas de faixas de valor.\n",
        "  \"\"\"\n",
        "  for col in colunas:\n",
        "    # Obter os valores da coluna\n",
        "    valores = df[col]\n",
        "\n",
        "    # 1. Calcular a amplitude total\n",
        "    L_min = valores.min()\n",
        "    L_max = valores.max()\n",
        "    AT = L_max - L_min\n",
        "\n",
        "    # 2. Determinar o número de classes\n",
        "    n = len(valores)\n",
        "    if metodo == 'sturges':\n",
        "        k = int(1 + 3.322 * np.log10(n))  # Regra de Sturges\n",
        "    elif metodo == 'sqrt':\n",
        "        k = int(np.sqrt(n))  # Regra da Raiz Quadrada\n",
        "    else:\n",
        "        raise ValueError(\"Método deve ser 'sturges' ou 'sqrt'.\")\n",
        "\n",
        "    # 3. Calcular a amplitude de cada classe\n",
        "    h = AT / k\n",
        "\n",
        "    # 4. Definir os limites das classes\n",
        "    faixas = []\n",
        "    for i in range(k):\n",
        "        lower_limit = L_min + i * h\n",
        "        upper_limit = lower_limit + h\n",
        "        faixa = f\"{lower_limit:.2f} - {upper_limit:.2f}\"\n",
        "        faixas.append(faixa)\n",
        "\n",
        "    # Adicionar a nova coluna de faixas ao DataFrame\n",
        "    df[f\"{col}_faixas\"] = pd.cut(df[col], bins=[-np.inf] + [float(limite.split('-')[0].strip()) for limite in faixas[:-1]] + [np.inf], labels=faixas)\n",
        "  if not implace: return df"
      ],
      "metadata": {
        "id": "IBEq6KzegpVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importando as faixas\n",
        "colunas = ['FASE', 'FAIXA_BANCO', 'FAIXA_FIM']\n",
        "\n",
        "#encoding='utf-8-sig #se parar de funcionar tentar esse charset'\n",
        "faixas_df = pd.read_csv(\n",
        "    input_path + 'dFaixas.csv',\n",
        "    encoding='latin-1',\n",
        "    sep=';',\n",
        "    low_memory=False,\n",
        "    dtype = str,\n",
        "    usecols = colunas\n",
        "  )\n",
        "faixas_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBE2o3kkTLht",
        "outputId": "954215a4-669e-4af0-dd50-3c63694302bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['FASE', 'FAIXA_BANCO', 'FAIXA_FIM'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importando e tratando a base dos meses\n",
        "base_columns = [\n",
        "    'CTT_OP', 'GRUPO_PERF', 'CARTEIRA','MARCACAO', 'FASE', 'UF', 'ATRASO_INI', 'FX_ATRASO_INI', 'VLCLSER',\n",
        "    'VL_LIQ_VENDA','TIPO_SUCESSO', 'DATA_PAGAMENTO', 'CAIXA_PERF', 'DT_ATRASO_INI','META_CC', 'META_RET', 'DATA_REF'\n",
        "]\n",
        "\n",
        "base_df = pd.read_csv(\n",
        "    input_path + 'bases.csv',\n",
        "    encoding='latin-1',\n",
        "    sep=';',\n",
        "    usecols = base_columns,\n",
        "    low_memory=False\n",
        ")\n",
        "\n",
        "base_df.drop_duplicates(inplace=True) #excluindo duplicatas\n",
        "\n",
        "# adicionando a faixa fim, do arquivo dfaixas.csv\n",
        "base_df = pd.merge(\n",
        "    base_df,\n",
        "    faixas_df,\n",
        "    left_on=['FASE', 'FX_ATRASO_INI'],\n",
        "    right_on=['FASE', 'FAIXA_BANCO'],\n",
        "    how='left',\n",
        ")\n",
        "\n",
        "#converte data numerica para o formato yyyy-mm-dd e depois para dd/mm/yyyy\n",
        "base_df['DATA_PAGAMENTO'] = pd.to_datetime(base_df['DATA_PAGAMENTO'], unit='D', origin='1899-12-30').dt.strftime('%d/%m/%Y')\n",
        "\n",
        "#adicionando a coluna de ANOMES para poder dar merge com df_sucessos\n",
        "base_df['DATA_REF'] = pd.to_datetime(base_df['DATA_REF'], dayfirst=True) #convertendo para data, nunca esquecer de passar dayfirst=True, pois senão troca dia e mes\n",
        "base_df['ANOMES'] = base_df['DATA_REF'].dt.strftime('%Y%m') #criando uma coluna com o ano e mês\n",
        "base_df = base_df.drop(columns=['DATA_REF','FX_ATRASO_INI'])\n",
        "\n",
        "colunas = {\n",
        "    'ATRASO_INI':'DIAS_ATRASO',\n",
        "    'VL_LIQ_VENDA':'VLV',\n",
        "    'DATA_PAGAMENTO': 'DT_SUC_BANCO',\n",
        "    'CAIXA_PERF':'VLR_PERF_BANCO',\n",
        "    'TIPO_SUCESSO':'TIPO_SUC_BANCO'\n",
        "}\n",
        "base_df.rename(columns=colunas, inplace=True)\n",
        "\n",
        "#gerando um dataframe transposto das colunas de FLAGS\n",
        "df_dummies = pd.get_dummies(base_df['MARCACAO'], prefix='FLAG', prefix_sep='_') # faz a tranposição, criando uma coluna para cada status\n",
        "df_dummies = df_dummies.astype(int) #mudando o retorno de true e false para 0 e 1\n",
        "base_df = pd.concat([base_df, df_dummies], axis=1) # juntando as colunas novas\n",
        "base_df = base_df.drop(columns=['MARCACAO','FLAG_ESTOQUE']) #excluindo a coluna redundante\n",
        "\n",
        "columns_to_fill = ['TIPO_SUC_BANCO','DT_SUC_BANCO', 'VLR_PERF_BANCO']\n",
        "base_df[columns_to_fill] = base_df[columns_to_fill].fillna('')\n",
        "\n",
        "base_df.columns"
      ],
      "metadata": {
        "id": "r6U5fiUOttfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06580440-fd0c-47a5-80f5-766a1fe45df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['CTT_OP', 'VLCLSER', 'CARTEIRA', 'FASE', 'UF', 'VLV', 'GRUPO_PERF',\n",
              "       'DIAS_ATRASO', 'TIPO_SUC_BANCO', 'DT_SUC_BANCO', 'VLR_PERF_BANCO',\n",
              "       'DT_ATRASO_INI', 'META_CC', 'META_RET', 'FAIXA_BANCO', 'FAIXA_FIM',\n",
              "       'ANOMES', 'FLAG_ENTRADA'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importando os daddos de HC\n",
        "colunas = ['LOGIN', 'STATUS', 'ESCOPO', 'SUPERVISOR', 'CLASS', 'ANOMES']\n",
        "\n",
        "#encoding='utf-8-sig #se parar de funcionar tentar esse charset'\n",
        "hc_df = pd.read_csv(\n",
        "    input_path + 'hc.csv',\n",
        "    encoding='latin-1',\n",
        "    sep=';',\n",
        "    low_memory=False,\n",
        "    dtype = str,\n",
        "    usecols = colunas\n",
        "  )\n",
        "\n",
        "#limpando textos indesejados\n",
        "limpar_colunas(hc_df, colunas)\n",
        "\n",
        "rename = {\n",
        "  'SUPERVISOR':'GESTOR',\n",
        "  'ESCOPO':'EQUIPE'\n",
        "}\n",
        "hc_df.rename(columns=rename, inplace=True)\n",
        "hc_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOaeidVdgFOw",
        "outputId": "30449ef2-8f60-449d-db95-4cd99a8aa4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['LOGIN', 'STATUS', 'EQUIPE', 'GESTOR', 'CLASS', 'ANOMES'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importando os sucessos\n",
        "colunas = ['CTT_OP', 'PARC', 'DATA_SUC', 'H.O', 'VLRDESP', 'VLRPG', 'ESP', 'OPER_FIM', 'NRO_BOL',\n",
        "'TP_SUC', 'FONTE', 'REPASSE', 'VLV', 'DATA_REF']\n",
        "\n",
        "#encoding='utf-8-sig #se parar de funcionar tentar esse charset'\n",
        "sucessos_df = pd.read_csv(\n",
        "    input_path + 'sucessos.csv',\n",
        "    encoding='latin-1',\n",
        "    sep=';',\n",
        "    low_memory=False,\n",
        "    dtype = str,\n",
        "    usecols = colunas\n",
        "  )\n",
        "\n",
        "sucessos_df.drop_duplicates(inplace=True) #excluindo duplicatas\n",
        "rename = {\n",
        "  'TP_SUC':'TIPO_SUCESSO',\n",
        "  'OPER_FIM': 'LOGIN',\n",
        "  'ESP': 'LOGIN_ESP',\n",
        "  'H.O': 'HO',\n",
        "  'NRO_BOL': 'NR_BOL',\n",
        "  'VLRPG': 'VLR_PAGO',\n",
        "  'VLRDESP': 'VLR_DESPESA'\n",
        "}\n",
        "sucessos_df.rename(columns=rename, inplace=True)\n",
        "\n",
        "#limpando textos indesejados\n",
        "colunas = ['CTT_OP','PARC','TIPO_SUCESSO','NR_BOL','LOGIN', 'LOGIN_ESP', 'VLR_DESPESA', 'VLR_PAGO','HO','REPASSE','VLV']\n",
        "limpar_colunas(sucessos_df, colunas)\n",
        "\n",
        "# Convertendo os tipos de dados\n",
        "colunas = ['VLR_PAGO','VLR_DESPESA','HO','REPASSE','VLV']\n",
        "to_numeric(sucessos_df, colunas)\n",
        "\n",
        "# renomeando os tipos de sucesso, para poder fazer a classificação\n",
        "sucessos_df['TIPO_SUCESSO'] = sucessos_df['TIPO_SUCESSO'].replace(\n",
        "    {\n",
        "    'apreensao': '01.BA',\n",
        "    'entrega': '02.EA',\n",
        "    'quitacao': '03.QUIT',\n",
        "    'refin': '04.REFIN',\n",
        "    'atualizacao': '05.ATUA',\n",
        "    'parcial': '06.PARC'\n",
        "  }\n",
        ")\n",
        "\n",
        "df_pagamento = sucessos_df[sucessos_df['PARC'] != ''] # Filtrar DataFrame com PARC não vazio\n",
        "df_demais = sucessos_df[sucessos_df['PARC'] == ''] # Filtrar DataFrame refin,retomado\n",
        "\n",
        "df_reduzido = df_pagamento.groupby('NR_BOL').agg({\n",
        "    'CTT_OP': 'first',\n",
        "    'PARC': lambda parcelas: ';'.join(str(p) for p in parcelas if pd.notna(p) and str(p) != ''),  # Convertendo para string\n",
        "    'DATA_SUC': 'first',\n",
        "    'HO': lambda ho: round(ho.sum(), 2),\n",
        "    'VLR_DESPESA': 'sum',\n",
        "    'VLR_PAGO': 'sum',\n",
        "    'LOGIN': 'first',\n",
        "    'LOGIN_ESP': 'first',\n",
        "    'TIPO_SUCESSO': 'first',\n",
        "    'FONTE': 'first',\n",
        "    'REPASSE': lambda repasse: round(repasse.sum(), 2),\n",
        "    'VLV': 'first',\n",
        "    'DATA_REF': 'first',\n",
        "}).reset_index()\n",
        "\n",
        "df_reduzido = pd.concat([df_reduzido, df_demais], ignore_index=True) #juntando pagamentos e demais sucessos\n",
        "\n",
        "#classificando os registros\n",
        "df_sucessos = df_reduzido.sort_values(\n",
        "    by=['CTT_OP', 'TIPO_SUCESSO','FONTE'],\n",
        "    ascending=[True, True,True]\n",
        ")\n",
        "\n",
        "#df_sucessos.drop_duplicates(subset=['CTT_OP', 'TIPO_SUCESSO'], keep='first', inplace=True) #mantendo apenas o melhor sucesso do contrato, que vai para a performance\n",
        "df_sucessos['TIPO_SUCESSO'] = df_sucessos['TIPO_SUCESSO'].str.slice(3) #extrai os caracteres a partir da posição 3, ou seja, do quarto em diante\n",
        "df_sucessos['DATA_REF'] = pd.to_datetime(df_sucessos['DATA_REF'], dayfirst=True) #convertendo para data\n",
        "df_sucessos['ANOMES'] = df_sucessos['DATA_REF'].dt.strftime('%Y%m') #criando uma coluna com o ano e mês\n",
        "df_sucessos = df_sucessos.drop(columns=['DATA_REF'])\n",
        "\n",
        "conditions = [\n",
        "  df_sucessos['TIPO_SUCESSO'] == 'EA',\n",
        "  df_sucessos['TIPO_SUCESSO'] == 'BA',\n",
        "  df_sucessos['REPASSE'] >= 0\n",
        "]\n",
        "\n",
        "status = [\n",
        "  df_sucessos['VLV'],\n",
        "  df_sucessos['VLV'],\n",
        "  df_sucessos['REPASSE']\n",
        "]\n",
        "\n",
        "df_sucessos['VLR_PERF'] = np.select(conditions, status, default=0)\n",
        "\n",
        "#gerando um dataframe transposto das colunas de FLAGS\n",
        "df_dummies = pd.get_dummies(df_sucessos['TIPO_SUCESSO'], prefix='FLAG', prefix_sep='_') # faz a tranposição, criando uma coluna para cada status\n",
        "df_dummies = df_dummies.astype(int) #mudando o retorno de true e false para 0 e 1\n",
        "df_sucessos = pd.concat([df_sucessos, df_dummies], axis=1) # juntando as colunas novas\n",
        "\n",
        "df_sucessos['FLAG_RET'] = df_sucessos['FLAG_EA'] + df_sucessos['FLAG_BA']\n",
        "df_sucessos['FLAG_PGTO'] = df_sucessos['FLAG_ATUA'] + df_sucessos['FLAG_PARC'] + df_sucessos['FLAG_QUIT']\n",
        "df_sucessos['FLAG_SUC'] = df_sucessos['FLAG_RET'] + df_sucessos['FLAG_PGTO'] + df_sucessos['FLAG_REFIN']\n",
        "\n",
        "#adicionando as colunas de HC\n",
        "df_sucessos = pd.merge(df_sucessos, hc_df[['LOGIN', 'EQUIPE', 'GESTOR', 'CLASS', 'ANOMES']],\n",
        "on=['LOGIN', 'ANOMES'], how='left', suffixes=('', '_NEW'))\n",
        "\n",
        "df_sucessos.loc[df_sucessos['LOGIN'] == 'nage',['EQUIPE','CLASS']] = 'retomada_judicial'\n",
        "columns_to_fill = ['EQUIPE','GESTOR', 'CLASS']\n",
        "df_sucessos[columns_to_fill] = df_sucessos[columns_to_fill].fillna('outros')\n",
        "\n",
        "df_sucessos['VLR_PERF'] = df_sucessos['VLR_PERF'].fillna(0)  # Preencher valores nulos com 0\n",
        "df_sucessos['VLR_PERF'] = df_sucessos['VLR_PERF'].astype(float)  # Converter para float\n",
        "\n",
        "#formatando a data de saída\n",
        "df_sucessos['DATA_SUC'] = pd.to_datetime(df_sucessos['DATA_SUC'], dayfirst=True)\n",
        "df_sucessos['DATA_SUC'] = df_sucessos['DATA_SUC'].dt.date\n",
        "\n",
        "df_sucessos.columns\n",
        "\n",
        "df_sucessos = pd.merge(\n",
        "    df_sucessos,\n",
        "    base_df[['CTT_OP', 'GRUPO_PERF', 'FASE', 'FAIXA_BANCO', 'FAIXA_FIM', 'ANOMES']],\n",
        "    on=['CTT_OP', 'ANOMES'],\n",
        "    how='left',\n",
        "    suffixes=('', '_INTERNO')\n",
        ")\n",
        "\n",
        "df_sucessos['FLAG_PERF'] = np.where((df_sucessos['FLAG_SUC'] > 0) & (df_sucessos['GRUPO_PERF'] == 'VEIC'), 1, 0) #definindo quais linhas irão performar\n",
        "\n",
        "df_sucessos.sort_values(by=['ANOMES','CTT_OP'], inplace=True) #classificando os dados, para agrupar os anomes\n",
        "ordenar_colunas = [\n",
        "  'CTT_OP', 'PARC', 'GRUPO_PERF', 'FASE', 'FAIXA_BANCO', 'FAIXA_FIM', 'LOGIN', 'LOGIN_ESP', 'DATA_SUC',\n",
        "  'NR_BOL', 'VLR_PAGO', 'VLR_DESPESA', 'HO', 'TIPO_SUCESSO', 'FONTE', 'GESTOR','EQUIPE', 'CLASS', 'REPASSE',\n",
        "  'VLV','VLR_PERF', 'FLAG_PERF', 'FLAG_REFIN', 'FLAG_RET', 'FLAG_BA', 'FLAG_EA', 'FLAG_PGTO','FLAG_QUIT','FLAG_ATUA', 'FLAG_PARC',\n",
        "  'FLAG_SUC', 'ANOMES'\n",
        "]\n",
        "df_sucessos = df_sucessos[ordenar_colunas]\n",
        "\n",
        "columns_to_fill = ['GRUPO_PERF','FASE', 'FAIXA_FIM']\n",
        "df_sucessos[columns_to_fill] = df_sucessos[columns_to_fill].fillna('BASE_OUT')\n",
        "\n",
        "#df_sucessos_columns = list(df_sucessos.columns)\n",
        "display(df_sucessos.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "u1s876SdgPnD",
        "outputId": "9a914174-f02b-49d0-d27e-70581d665118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Index(['CTT_OP', 'PARC', 'GRUPO_PERF', 'FASE', 'FAIXA_BANCO', 'FAIXA_FIM',\n",
              "       'LOGIN', 'LOGIN_ESP', 'DATA_SUC', 'NR_BOL', 'VLR_PAGO', 'VLR_DESPESA',\n",
              "       'HO', 'TIPO_SUCESSO', 'FONTE', 'GESTOR', 'EQUIPE', 'CLASS', 'REPASSE',\n",
              "       'VLV', 'VLR_PERF', 'FLAG_PERF', 'FLAG_REFIN', 'FLAG_RET', 'FLAG_BA',\n",
              "       'FLAG_EA', 'FLAG_PGTO', 'FLAG_QUIT', 'FLAG_ATUA', 'FLAG_PARC',\n",
              "       'FLAG_SUC', 'ANOMES'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################## CRIANDO O OUTPUT DE SUCESSOS [sucessos_2024.csv] ##########################\n",
        "\n",
        "#trocando os pontos por virgula para deixar já no formato de exportação excel correto\n",
        "output_sucessos = df_sucessos.map(lambda x: str(x).replace('.', ','))\n",
        "output_sucessos[['FAIXA_BANCO', 'FAIXA_FIM']] = output_sucessos[['FAIXA_BANCO', 'FAIXA_FIM']].apply(lambda x: x.str.replace(',', '.')) #corrigindo as faixas\n",
        "\n",
        "#definindo colunas de saida\n",
        "output_columns = [\n",
        "    'CTT_OP', 'PARC', 'GRUPO_PERF', 'FASE', 'FAIXA_BANCO', 'FAIXA_FIM', 'LOGIN', 'LOGIN_ESP', 'DATA_SUC',\n",
        "    'NR_BOL', 'VLR_PAGO', 'VLR_DESPESA', 'HO', 'TIPO_SUCESSO', 'FONTE', 'GESTOR','EQUIPE', 'CLASS', 'REPASSE',\n",
        "    'VLV','VLR_PERF', 'FLAG_PERF', 'FLAG_REFIN', 'FLAG_RET', 'FLAG_BA', 'FLAG_EA', 'FLAG_PGTO','FLAG_QUIT','FLAG_ATUA', 'FLAG_PARC',\n",
        "      'FLAG_SUC', 'ANOMES'\n",
        "]\n",
        "\n",
        "output_sucessos = output_sucessos[output_columns]\n",
        "export_to_csv(output_sucessos, output_path,file_name='sucessos_2024.csv',index=False)\n",
        "output_sucessos.columns"
      ],
      "metadata": {
        "id": "QDol0O5vVhn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40213cc3-26ba-4775-a5d2-6122c0ddc9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['CTT_OP', 'PARC', 'GRUPO_PERF', 'FASE', 'FAIXA_BANCO', 'FAIXA_FIM',\n",
              "       'LOGIN', 'LOGIN_ESP', 'DATA_SUC', 'NR_BOL', 'VLR_PAGO', 'VLR_DESPESA',\n",
              "       'HO', 'TIPO_SUCESSO', 'FONTE', 'GESTOR', 'EQUIPE', 'CLASS', 'REPASSE',\n",
              "       'VLV', 'VLR_PERF', 'FLAG_PERF', 'FLAG_REFIN', 'FLAG_RET', 'FLAG_BA',\n",
              "       'FLAG_EA', 'FLAG_PGTO', 'FLAG_QUIT', 'FLAG_ATUA', 'FLAG_PARC',\n",
              "       'FLAG_SUC', 'ANOMES'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################## GERANDO AS COLUNAS DE SUCESSO ESCOB PARA COMPARAR COM OS DADOS DO BANCO ##########################\n",
        "\n",
        "colunas_necessarias = ['CTT_OP','TIPO_SUCESSO','VLR_PERF', 'FLAG_PERF', 'ANOMES']\n",
        "sucesso_resumido = df_sucessos[colunas_necessarias]\n",
        "\n",
        "colunas = [\n",
        "    'CTT_OP', 'GRUPO_PERF', 'CARTEIRA', 'FASE', 'UF', 'FLAG_ENTRADA', 'DT_ATRASO_INI', 'DIAS_ATRASO', 'FAIXA_BANCO', 'FAIXA_FIM', 'VLCLSER','META_CC', 'META_RET',\n",
        "    'VLV','TIPO_SUC_BANCO', 'DT_SUC_BANCO', 'VLR_PERF_BANCO',  'ANOMES'\n",
        "]  # Colunas que você deseja manter\n",
        "base_final = base_df[colunas]\n",
        "\n",
        "filtro_tp_suc = sucesso_resumido.drop_duplicates(subset=['CTT_OP', 'ANOMES'], keep='first')\n",
        "df_ret = filtro_tp_suc[['CTT_OP','TIPO_SUCESSO', 'ANOMES']]\n",
        "df_ret.rename(columns={'TIPO_SUCESSO': 'TP_SUC_INTERNO'}, inplace=True)\n",
        "\n",
        "base_final = pd.merge(\n",
        "    base_final,\n",
        "    df_ret,\n",
        "    left_on=['CTT_OP', 'ANOMES'],\n",
        "    right_on=['CTT_OP', 'ANOMES'],\n",
        "    how='left',\n",
        ")\n",
        "\n",
        "#breakpoint()\n",
        "filtro_pagto = sucesso_resumido[~sucesso_resumido['TIPO_SUCESSO'].isin(['EA', 'BA', 'REFIN'])]\n",
        "df_ret = round(filtro_pagto.groupby(['CTT_OP', 'ANOMES'])['VLR_PERF'].sum().reset_index(),2)\n",
        "df_ret.rename(columns={'VLR_PERF': 'PERF_PGTO_ESCOB'}, inplace=True)\n",
        "\n",
        "base_final = pd.merge(\n",
        "    base_final,\n",
        "    df_ret[['CTT_OP', 'ANOMES','PERF_PGTO_ESCOB']],\n",
        "    left_on=['CTT_OP', 'ANOMES'],\n",
        "    right_on=['CTT_OP', 'ANOMES'],\n",
        "    how='left',\n",
        ")\n",
        "\n",
        "filtro_ret = sucesso_resumido[sucesso_resumido['TIPO_SUCESSO'].isin(['EA', 'BA'])]\n",
        "df_ret = round(filtro_ret.groupby(['CTT_OP', 'ANOMES'])['VLR_PERF'].sum().reset_index(),2)\n",
        "df_ret.rename(columns={'VLR_PERF': 'PERF_RET_ESCOB'}, inplace=True)\n",
        "\n",
        "base_final = pd.merge(\n",
        "    base_final,\n",
        "    df_ret[['CTT_OP', 'ANOMES','PERF_RET_ESCOB']],\n",
        "    left_on=['CTT_OP', 'ANOMES'],\n",
        "    right_on=['CTT_OP', 'ANOMES'],\n",
        "    how='left',\n",
        ")\n",
        "\n",
        "filtro_refin = sucesso_resumido[sucesso_resumido['TIPO_SUCESSO'].isin(['REFIN'])]\n",
        "df_ret = round(filtro_refin.groupby(['CTT_OP', 'ANOMES'])['VLR_PERF'].count().reset_index(),2)\n",
        "df_ret.rename(columns={'VLR_PERF': 'PERF_REFIN_ESCOB'}, inplace=True)\n",
        "\n",
        "base_final = pd.merge(\n",
        "    base_final,\n",
        "    df_ret[['CTT_OP', 'ANOMES','PERF_REFIN_ESCOB']],\n",
        "    left_on=['CTT_OP', 'ANOMES'],\n",
        "    right_on=['CTT_OP', 'ANOMES'],\n",
        "    how='left',\n",
        ")\n",
        "\n",
        "base_final['PERF_INTERNA'] = np.where(\n",
        "  base_final['PERF_RET_ESCOB']>0,\n",
        "  base_final['PERF_RET_ESCOB'],\n",
        "  np.where(\n",
        "    base_final['PERF_PGTO_ESCOB'] > 0,\n",
        "    base_final['PERF_PGTO_ESCOB'],\n",
        "    0\n",
        "  )\n",
        ")\n",
        "\n",
        "base_final['FLAG_PERF'] = np.where((base_final['PERF_REFIN_ESCOB'] > 0) | (base_final['PERF_INTERNA'] > 0), 1, 0) #definindo quais linhas irão performar\n",
        "\n",
        "#export_to_csv(base_final, output_path)\n",
        "base_final.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssGhetqWrIX2",
        "outputId": "b06215b9-b1af-4a0f-d925-eff01e203c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-03be401745ba>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_ret.rename(columns={'TIPO_SUCESSO': 'TP_SUC_INTERNO'}, inplace=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['CTT_OP', 'GRUPO_PERF', 'CARTEIRA', 'FASE', 'UF', 'FLAG_ENTRADA',\n",
              "       'DT_ATRASO_INI', 'DIAS_ATRASO', 'FAIXA_BANCO', 'FAIXA_FIM', 'VLCLSER',\n",
              "       'META_CC', 'META_RET', 'VLV', 'TIPO_SUC_BANCO', 'DT_SUC_BANCO',\n",
              "       'VLR_PERF_BANCO', 'ANOMES', 'TP_SUC_INTERNO', 'PERF_PGTO_ESCOB',\n",
              "       'PERF_RET_ESCOB', 'PERF_REFIN_ESCOB', 'PERF_INTERNA', 'FLAG_PERF'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################## GERANDO O OUTPUT DA BASE FINAL [ BASE + SUCESSO_ESCOB ] ##########################\n",
        "\n",
        "output_columns = [\n",
        "  'CTT_OP', 'GRUPO_PERF', 'CARTEIRA', 'FASE', 'UF', 'DT_ATRASO_INI', 'DIAS_ATRASO', 'FAIXA_BANCO', 'FAIXA_FIM',\n",
        "  'VLCLSER',  'META_CC', 'META_RET',  'VLV','TIPO_SUC_BANCO', 'DT_SUC_BANCO', 'VLR_PERF_BANCO', 'FLAG_ENTRADA',\n",
        "  'TP_SUC_INTERNO', 'PERF_INTERNA', 'PERF_PGTO_ESCOB', 'PERF_RET_ESCOB', 'PERF_REFIN_ESCOB', 'FLAG_PERF', 'ANOMES'\n",
        "]  # Colunas que você deseja manter\n",
        "output_base = base_final[output_columns] #reordenando as colunas\n",
        "\n",
        "# Convertendo os tipos de dados\n",
        "colunas = ['PERF_INTERNA','PERF_PGTO_ESCOB', 'PERF_RET_ESCOB', 'PERF_REFIN_ESCOB']\n",
        "to_numeric(output_base, colunas)\n",
        "output_base = output_base.sort_values(by=['ANOMES', 'PERF_INTERNA'], ascending=[False, False])\n",
        "\n",
        "output_base = output_base.map(lambda x: str(x).replace('.', ',').replace('nan', ''))\n",
        "output_base[['FAIXA_BANCO', 'FAIXA_FIM']] = output_base[['FAIXA_BANCO', 'FAIXA_FIM']].apply(lambda x: x.str.replace(',', '.')) #corrigindo as faixas\n",
        "export_to_csv(output_base, output_path,file_name='base_final_2024.csv',index=False)"
      ],
      "metadata": {
        "id": "SKiWepCNqYL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_numeric(base_df, ['CAIXA_PERF'])\n",
        "to_numeric(base_df, ['VLR_PERF'])\n",
        "caixa_banco = round(base_df['CAIXA_PERF']/100, 0) * 100\n",
        "caixa_interno = round(base_df['VLR_PERF']/100, 0) * 100\n",
        "\n",
        "conditions = [\n",
        "  caixa_banco == caixa_interno,\n",
        "  caixa_banco > caixa_interno,\n",
        "  caixa_banco < caixa_interno,\n",
        "  pd.isna(base_df['TIPO_SUCESSO'])\n",
        "]\n",
        "\n",
        "status = [\n",
        "  'IGUAL',\n",
        "  'MAIOR',\n",
        "  'MENOR',\n",
        "  'FALTANDO PERF'\n",
        "]\n",
        "base_df['ST_PERF'] = np.select(conditions, status, default='FALTANDO PERF')\n",
        "\n",
        "to_numeric(base_df, ['VLR_REPASSE']) # elimna também os nan\n",
        "base_df['VLR_REPASSE'] = base_df['VLR_REPASSE'].astype(str).str.replace('.', ',', regex=False)\n",
        "#display(base_df.head())\n",
        "#base_df = base_df.head(10000)\n"
      ],
      "metadata": {
        "id": "EiBvfxquPfCo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "82dcbaaf-b452-4ae6-ca1b-aa1dd9866a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'CAIXA_PERF'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'CAIXA_PERF'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9f6934bbdca6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'CAIXA_PERF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'VLR_PERF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcaixa_banco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CAIXA_PERF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcaixa_interno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VLR_PERF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-11b0fefc6132>\u001b[0m in \u001b[0;36mto_numeric\u001b[0;34m(df, colunas)\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolunas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     df[col] = (\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     ).fillna(0)\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3893\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3796\u001b[0m             ):\n\u001b[1;32m   3797\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3798\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3799\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'CAIXA_PERF'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ed1ObBKKPqjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar a tabela de resumo\n",
        "tabela_resumo = resumo_suc.pivot_table(\n",
        "    values='VLR_PERF',\n",
        "    index='EQUIPE',\n",
        "    columns='ANOMES',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "tabela_resumo = tabela_resumo.apply(lambda x: x / x.sum() * 100, axis=0) # Calcular a porcentagem de sucesso para cada equipe e mês\n",
        "tabela_resumo['Total'] = tabela_resumo.sum(axis=1) / tabela_resumo.shape[1]\n",
        "tabela_resumo.fillna(0, inplace=True)\n",
        "tabela_resumo = tabela_resumo.map(lambda x: f\"{x:.2f}%\")\n",
        "tabela_resumo.index.name = 'EQUIPE'\n",
        "tabela_resumo = tabela_resumo.map(lambda x: str(x).replace('.', ','))\n",
        "\n",
        "#export_to_csv(tabela_resumo, output_path,file_name='equipe_vs_ano.csv',index=True)"
      ],
      "metadata": {
        "id": "h5fsMBgYGDOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKEAeE1YX0zx"
      },
      "outputs": [],
      "source": [
        "#cria flags comparando duas tabelas\n",
        "def flagCreator(col_flag_name, main_df, df_to_filter, merge_columns, lambda_conditions=None):\n",
        "    \"\"\"Cria uma coluna de flag binária em main_df com base em uma condição de filtro em df_to_filter.\n",
        "    Args:\n",
        "        col_flag_name (str): Nome da coluna de flag a ser criada em main_df.\n",
        "        main_df (pd.DataFrame): DataFrame principal onde a flag será criada.\n",
        "        df_to_filter (pd.DataFrame): DataFrame a ser filtrado para encontrar correspondências.\n",
        "        merge_columns (list): Lista de colunas a serem usadas para o merge.\n",
        "        lambda_conditions (function, optional): Função lambda que retorna True para as linhas que devem ser incluídas no filtro.\n",
        "            Se não for fornecida, todas as linhas de df_to_filter serão usadas. Defaults to None.\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame main_df com a coluna de flag adicionada.\n",
        "    \"\"\"\n",
        "\n",
        "    main_df[col_flag_name] = 0  # Inicializa a coluna com 0\n",
        "\n",
        "    # Se lambda_conditions não for fornecido, use todas as linhas de df_to_filter\n",
        "    if lambda_conditions is None:\n",
        "        filtered_df = df_to_filter[merge_columns]\n",
        "    else:\n",
        "        filtered_df = df_to_filter[lambda_conditions(df_to_filter)][merge_columns]\n",
        "\n",
        "    base_df = main_df.merge(\n",
        "        filtered_df,\n",
        "        on=merge_columns,\n",
        "        left_on=['CTT_OP', 'DATA_REF'],\n",
        "        right_on=['CTT_OP', 'DATA_REF'],\n",
        "        how='left',\n",
        "        indicator=True\n",
        "    )\n",
        "    main_df.loc[base_df['_merge'] == 'both', col_flag_name] = 1  # Atualiza a flag para 1 onde houve correspondência\n",
        "    return main_df\n",
        "\n",
        "bases_df = flagCreator( 'FLAG_SUC', bases_df, sucessos_df, ['CTT_OP', 'DATA_REF']) # Cria FLAG_SUC\n",
        "bases_df = flagCreator('FLAG_EA', bases_df, sucessos_df, ['CTT_OP', 'DATA_REF'], lambda sucessos_df: sucessos_df['TP_SUC'] == 'Entrega') # Cria FLAG_EA\n",
        "bases_df = flagCreator( 'FLAG_BA', bases_df, sucessos_df, ['CTT_OP', 'DATA_REF'], lambda sucessos_df: sucessos_df['TP_SUC'] == 'Apreensão') # Cria FLAG_BA\n",
        "bases_df.to_csv('/content/drive/MyDrive/bases_df.csv', index=False, sep=';', encoding='utf-8-sig')\n",
        "bases_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82jc5GwLx0y-"
      },
      "outputs": [],
      "source": [
        "#bases_df[['CTT_OP','']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrSzUNIAtLkO"
      },
      "outputs": [],
      "source": [
        "def comentar():\n",
        "\n",
        "  bases_df['FLAG_SUC'] = 0 # Adiciona a coluna 'FLAG_SUC' inicialmente com valor 0\n",
        "  base_df_suc = bases_df.merge(sucessos_df[['CTT_OP', 'DATA_REF']], left_on=['CTT_OP', 'DATA_REF'], how='left', indicator=True)\n",
        "  # Atualiza a coluna FLAG_SUC para 1 onde houve correspondência\n",
        "  bases_df['FLAG_SUC'] = (base_df_suc['_merge'] == 'both').astype(int)  #cria uma coluna chamada '_merge' com dois retornos possíveis, left_only e both\n",
        "\n",
        "  bases_df['FLAG_EA'] = 0 # Adiciona a coluna 'FLAG_EA' inicialmente com valor 0\n",
        "  base_df_ea = bases_df.merge(\n",
        "      sucessos_df[sucessos_df['TP_SUC'] == 'Entrega'][['CTT_OP', 'DATA_REF']], #filtra as linhas = entregas, filtra as colunas 'CTT_OP' e 'DATA_REF'\n",
        "      on=['CTT_OP', 'DATA_REF'],\n",
        "      how='left',\n",
        "      indicator=True\n",
        "  )\n",
        "  bases_df.loc[base_df_ea['_merge'] == 'both', 'FLAG_EA'] = 1 # Atualiza a coluna FLAG_EA para 1 onde houve correspondência com TP_SUC == 'Entrega'\n",
        "\n",
        "  bases_df['FLAG_BA'] = 0 # Adiciona a coluna 'FLAG_EA' inicialmente com valor 0\n",
        "  base_df_ea = bases_df.merge(sucessos_df[sucessos_df['TP_SUC'] == 'Apreensão'][['CTT_OP', 'DATA_REF']], on=['CTT_OP', 'DATA_REF'], how='left', indicator=True)\n",
        "  bases_df.loc[base_df_ea['_merge'] == 'both', 'FLAG_BA'] = 1 # Atualiza a coluna FLAG_EA para 1 onde houve correspondência com TP_SUC == 'Entrega'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqIMXKhoy6ci"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWMt4Wv6Dp8Q"
      },
      "outputs": [],
      "source": [
        "agrupado = bases_df.groupby(['DATA_REF', 'TIPO_SUC_FIM'])['FLAG_SUC'].sum().unstack()\n",
        "agrupado['TOTAL'] = agrupado.sum(axis=1)\n",
        "display(agrupado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHOJL8Fj-UT"
      },
      "outputs": [],
      "source": [
        "#sucessos_df = pd.merge(sucessos_df, bases_df[['CTT_OP', 'DATA_REF', 'FASE']], on=['CTT_OP', 'DATA_REF'], how='left') #adicionando a coluna de fase\n",
        "#sucessos_df.to_csv('/content/drive/MyDrive/sucessos_com_fase.csv', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5FtZWgsPYby"
      },
      "outputs": [],
      "source": [
        "def amostragem_estratificada(df, group_columns, perc_member):\n",
        "  \"\"\"\n",
        "  Realiza amostragem estratificada em um DataFrame com base em múltiplos níveis de agrupamento.\n",
        "  Args:\n",
        "      df (pd.DataFrame): DataFrame de entrada.\n",
        "      group_columns (list): Lista de nomes de colunas para agrupar.\n",
        "      perc_member (float): Porcentagem de elementos a serem amostrados dentro de cada grupo.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: DataFrame com a amostra estratificada.\n",
        "  \"\"\"\n",
        "  # Aplicar a amostragem aleatória em cada grupo\n",
        "  aux = df.groupby(group_columns, group_keys=False).apply(lambda x: x.sample(frac=perc_member))\n",
        "  return aux\n",
        "\n",
        "#amostra = amostragem_estratificada(sucessos_df,['DATA_REF','FAIXA_ATRASO'],0.5)\n",
        "#amostra.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVEvVbhPOLgA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}